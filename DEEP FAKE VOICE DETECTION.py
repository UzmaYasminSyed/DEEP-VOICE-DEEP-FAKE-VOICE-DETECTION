# -*- coding: utf-8 -*-
"""DEEP_FAKE_VOICE_PROJECT 3-2203A52055.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aezYi_DpdefnVOo0GCYvBBwlEFpGNx3h
"""

# prompt: drive code

from google.colab import drive
drive.mount('/content/drive')

pip install librosa numpy pandas scikit-learn tensorflow matplotlib soundfile

data_dir='/content/drive/MyDrive/Deep Fake Voice '

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Path to your dataset directory in Drive
data_dir = '/content/drive/MyDrive/Deep Fake Voice '

# Check that the directory exists and list files
if os.path.exists(data_dir):
    print("Dataset folder found at:", data_dir)
    print("Files inside:")
    for root, dirs, files in os.walk(data_dir):
        for name in files:
            print(os.path.join(root, name))
else:
    print("Dataset folder not found. Please check the path.")

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
import IPython.display as ipd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# -------------------- CONFIG --------------------
DATA_DIR = 'data'  # Directory with 'real/' and 'fake/' folders

# -------------------- AUDIO FEATURE EXTRACTION --------------------
def extract_mfcc(file_path, n_mfcc=20):
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    mfccs_mean = np.mean(mfccs.T, axis=0)
    return mfccs_mean

# -------------------- LOAD DATA --------------------
def load_data(data_dir):
    features = []
    labels = []
    audio_samples = []

    for label in ['real', 'fake']:
        dir_path = os.path.join(data_dir, label)
        if not os.path.exists(dir_path):
            print(f"Warning: {dir_path} does not exist!")
            continue

        for fname in os.listdir(dir_path):
            if fname.endswith(".wav"):
                file_path = os.path.join(dir_path, fname)
                try:
                    mfcc = extract_mfcc(file_path)
                    features.append(mfcc)
                    labels.append(label)
                    audio_samples.append(file_path)
                except Exception as e:
                    print(f"Failed to process {file_path}: {e}")

    return np.array(features), np.array(labels), audio_samples

# -------------------- MAIN --------------------
if __name__ == "__main__":
    X, y, audio_paths = load_data(DATA_DIR)

    if len(X) == 0:
        print("No audio files found. Check your data directory.")
        exit()

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test, audio_train, audio_test = train_test_split(
        X, y_encoded, audio_paths, test_size=0.2, random_state=42
    )

    # Train model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Evaluation
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    print("Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

    # -------------------- LISTEN TO TEST AUDIO --------------------
    print("\nListening to some test samples:")
    for i in range(min(3, len(audio_test))):
        print(f"\nSample {i+1}: Predicted - {le.inverse_transform([y_pred[i]])[0]}, Actual - {le.inverse_transform([y_test[i]])[0]}")
        ipd.display(ipd.Audio(audio_test[i]))

import os
import librosa
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

# Feature extraction
def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    features = np.concatenate((np.mean(mfccs, axis=1),
                               np.mean(chroma, axis=1),
                               np.mean(contrast, axis=1)))
    return features

# Load dataset
def load_dataset(data_dir):
    features, labels = [], []
    for label in ['real', 'fake']:
        dir_path = os.path.join(data_dir, label)
        for file_name in os.listdir(dir_path):
            file_path = os.path.join(dir_path, file_name)
            try:
                feat = extract_features(file_path)
                features.append(feat)
                labels.append(label)
            except Exception as e:
                print(f"Error with file {file_path}: {e}")
    return np.array(features), np.array(labels)

# Build CNN + LSTM model
def build_model(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.Reshape((input_shape[0], 1), input_shape=input_shape),
        tf.keras.layers.Conv1D(64, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.Conv1D(128, 3, activation='relu'),
        tf.keras.layers.MaxPooling1D(2),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Plot training history
def plot_history(history):
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='val accuracy')
    plt.title("Training History")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

# Main
if __name__ == "__main__":
    data_dir = 'data'
    X, y = load_dataset(data_dir)

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    model = build_model(X_train.shape[1:])
    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

    plot_history(history)

    # Evaluation
    y_pred = (model.predict(X_test) > 0.5).astype("int32")
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    model.save('models/deep_voice_model.h5')

import librosa
import numpy as np
import os

def extract_features(file_path):
    """
    Extracts audio features from a given file.
    """
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    features = np.concatenate((np.mean(mfccs, axis=1),
                               np.mean(chroma, axis=1),
                               np.mean(contrast, axis=1)))
    return features

# Assuming your audio files are within the 'FAKE' directory
# Adjust the path if needed
audio_dir = "/content/drive/MyDrive/Deep Fake Voice /Deep Fake Voice /KAGGLE/AUDIO/FAKE"

# Loop through all files in the directory
for filename in os.listdir(audio_dir):
    if filename.endswith(".wav"):  # Consider only .wav files
        file_path = os.path.join(audio_dir, filename)

        # Call the function for each audio file
        features = extract_features(file_path)
        # Now you can use the 'features' for further processing
        print(f"Features extracted for: {filename}")

import librosa
import numpy as np
import os

def extract_features(file_path):
    """
    Extracts audio features from a given file.
    """
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    features = np.concatenate((np.mean(mfccs, axis=1),
                               np.mean(chroma, axis=1),
                               np.mean(contrast, axis=1)))
    return features

# Assuming your audio files are within the 'FAKE' directory
# Adjust the path if needed
audio_dir = "/content/drive/MyDrive/Deep Fake Voice /Deep Fake Voice /KAGGLE/AUDIO/REAL"

# Loop through all files in the directory
for filename in os.listdir(audio_dir):
    if filename.endswith(".wav"):  # Consider only .wav files
        file_path = os.path.join(audio_dir, filename)

        # Call the function for each audio file
        features = extract_features(file_path)
        # Now you can use the 'features' for further processing
        print(f"Features extracted for: {filename}")

# Imagine these are very basic versions of real tools

def get_voice_fingerprint(audio_file):
  """Listens to audio and creates a voice ID."""
  print(f"Getting voice ID from {audio_file}")
  return ["voice", "sound", "unique"] # A very simplified ID

def say_with_voice(text, voice_id):
  """Says the text using the given voice ID."""
  print(f"Saying '{text}' with voice ID: {voice_id}")
  # In reality, this would generate audio
  return f"Audio of '{text}' in a special voice"

reference_audio = "reference_voice.wav"
text = "Hello, simple voice copy."

voice_id = get_voice_fingerprint(reference_audio)
output_audio = say_with_voice(text, voice_id)

print(output_audio)

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# 1. Load audio and convert to mel-spectrogram
audio_path = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE/biden-to-Trump.wav"
y, sr = librosa.load(audio_path)
mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
mel_spectrogram_tensor = torch.tensor(mel_spectrogram_db).unsqueeze(0).unsqueeze(1).float() # (batch, channel, freq, time)

# 2. Define a simple CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(32 * (mel_spectrogram_tensor.shape[2] // 4) * (mel_spectrogram_tensor.shape[3] // 4), 128) # Example output size

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.flatten(x)
        x = self.fc(x)
        return x

cnn_model = SimpleCNN()
output_features = cnn_model(mel_spectrogram_tensor)
print("CNN Output Features Shape:", output_features.shape)

import librosa
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# --- Data Paths ---
real_data_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL/obama-original.wav"  # Replace with the actual path to your real audio files
fake_data_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE/trump-to-Obama.wav"  # Replace with the actual path to your fake audio files

# --- Feature Extraction Function ---
def extract_features(audio_path, duration=3):
    """Extracts MFCC features from an audio file."""
    try:
        y, sr = librosa.load(audio_path, duration=duration)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
        return np.mean(mfccs.T, axis=0)
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None

# --- Load Data and Extract Features ---
real_features = []
fake_features = []
real_labels = []
fake_labels = []

# Load real audio and extract features
if os.path.isdir(real_data_dir):
    for filename in os.listdir(real_data_dir):
        if filename.endswith(".wav"):
            audio_path = os.path.join(real_data_dir, filename)
            features = extract_features(audio_path)
            if features is not None:
                real_features.append(features)
                real_labels.append(0)  # 0 for real

# Load fake audio and extract features
if os.path.isdir(fake_data_dir):
    for filename in os.listdir(fake_data_dir):
        if filename.endswith(".wav"):
            audio_path = os.path.join(fake_data_dir, filename)
            features = extract_features(audio_path)
            if features is not None:
                fake_features.append(features)
                fake_labels.append(1)  # 1 for fake

# Combine features and labels
all_features = np.array(real_features + fake_features)
all_labels = np.array(real_labels + fake_labels)

# --- Data Preprocessing ---
if all_features.shape[0] > 0:
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42, stratify=all_labels)

    # Feature scaling (important for SVM)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # --- Initialize and Train the SVM Classifier ---
    svm_classifier = SVC(kernel='rbf', C=1.0, probability=True) # Added probability=True for potential probability estimates
    svm_classifier.fit(X_train_scaled, y_train)

    # --- Make Predictions and Evaluate ---
    y_pred = svm_classifier.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # You can also get probability estimates if needed
    y_pred_proba = svm_classifier.predict_proba(X_test_scaled)[:, 1] # Probability of being fake
    # print("Predicted Probabilities (for 'fake' class):", y_pred_proba)

else:
    print("No audio files found in the specified directories.")
    print(f"Real directory: {real_data_dir}")
    print(f"Fake directory: {fake_data_dir}")
    print("Please make sure these directories exist and contain '.wav' audio files.")

import librosa
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import soundfile as sf
import IPython.display as ipd

# --- 1. Define Data Paths ---
# --- 1. Define Data Paths ---
real_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL"  # Make sure this is the DIRECTORY
fake_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE"  # Make sure this is the DIRECTORY

# --- 2. Function to Extract Features from a Single Audio File ---
def extract_features(audio_path):
    try:
        y, sr = librosa.load(audio_path, duration=3)  # Example duration
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
        return np.mean(mfccs.T, axis=0)  # Average MFCCs over time
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None

# --- 3. Load Data and Extract Features from Directories ---
real_features = []
fake_features = []
real_files_processed = 0
fake_files_processed = 0

for filename in os.listdir(real_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(real_audio_dir, filename)
        features = extract_features(audio_path)
        if features is not None:
            real_features.append(features)
            real_files_processed += 1

for filename in os.listdir(fake_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(fake_audio_dir, filename)
        features = extract_features(audio_path)
        if features is not None:
            fake_features.append(features)
            fake_files_processed += 1

print(f"Processed {real_files_processed} real audio files.")
print(f"Processed {fake_files_processed} fake audio files.")

# --- 4. Create Labels and Combine Features ---
labels = [0] * len(real_features) + [1] * len(fake_features)  # 0: real, 1: fake
features = real_features + fake_features

# Convert to numpy arrays
if not features:
    print("No audio features were extracted. Please check your data paths and audio files.")
    exit()

X = np.array(features)
y = np.array(labels)

# --- 5. Split Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- 6. Feature Scaling ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- 7. Initialize and Train the SVM Classifier ---
svm_classifier = SVC(kernel='rbf', C=1.0, probability=True)  # Enable probability estimates
svm_classifier.fit(X_train_scaled, y_train)

# --- 8. Function to Predict and Play a Single Audio File ---
def predict_and_play(audio_path, scaler, model):
    features = extract_features(audio_path)
    if features is not None:
        scaled_features = scaler.transform(features.reshape(1, -1))
        prediction = model.predict(scaled_features)[0]
        probability = model.predict_proba(scaled_features)[0][1] # Probability of being fake

        if prediction == 0:
            result = "REAL"
        else:
            result = "FAKE"

        print(f"Audio: {os.path.basename(audio_path)}")
        print(f"Prediction: {result} (Probability of Fake: {probability:.2f})")
        ipd.display(ipd.Audio(audio_path))
    else:
        print(f"Could not process audio file: {os.path.basename(audio_path)}")

# --- 9. Example Usage for a Specific File ---
specific_audio_path = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL/obama-original.wav"
predict_and_play(specific_audio_path, scaler, svm_classifier)

# --- 10. Evaluate Overall Performance (Optional) ---
y_pred = svm_classifier.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nOverall Accuracy: {accuracy:.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import os

# --- 1. Define Data Paths ---
real_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL"
fake_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE"

# --- 2. Function to Load Audio and Display Waveform ---
def plot_waveform(audio_path, title):
    try:
        y, sr = librosa.load(audio_path)
        plt.figure(figsize=(10, 4))
        librosa.display.waveshow(y, sr=sr)
        plt.title(title)
        plt.xlabel("Time (s)")
        plt.ylabel("Amplitude")
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print(f"Error loading or displaying waveform for {audio_path}: {e}")

# --- 3. Get a Sample Real Audio File ---
real_audio_files = [f for f in os.listdir(real_audio_dir) if f.endswith(".wav")]
if real_audio_files:
    sample_real_path = os.path.join(real_audio_dir, real_audio_files[0])
    plot_waveform(sample_real_path, "Waveform of a Real Audio Sample")
else:
    print(f"No .wav files found in the real audio directory: {real_audio_dir}")

# --- 4. Get a Sample Fake Audio File ---
fake_audio_files = [f for f in os.listdir(fake_audio_dir) if f.endswith(".wav")]
if fake_audio_files:
    sample_fake_path = os.path.join(fake_audio_dir, fake_audio_files[0])
    plot_waveform(sample_fake_path, "Waveform of a Fake Audio Sample")
else:
    print(f"No .wav files found in the fake audio directory: {fake_audio_dir}")

import librosa
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# --- 1. Define Data Paths ---
real_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL"
fake_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE"

# --- 2. Function to Extract Sequential Features (e.g., MFCCs) ---
def extract_sequential_features(audio_path, n_mfcc=20, frame_length=2048, hop_length=512, max_len=150):
    try:
        y, sr = librosa.load(audio_path, duration=3)  # Example duration
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=frame_length, hop_length=hop_length)
        mfccs_transposed = mfccs.T

        # Pad or truncate sequences to a fixed length
        if mfccs_transposed.shape[0] < max_len:
            padding = np.zeros((max_len - mfccs_transposed.shape[0], n_mfcc))
            return np.vstack((mfccs_transposed, padding))
        else:
            return mfccs_transposed[:max_len]
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None

# --- 3. Load Data and Extract Features ---
real_features = []
fake_features = []
labels = []

for filename in os.listdir(real_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(real_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            real_features.append(features)
            labels.append(0)  # 0: real

for filename in os.listdir(fake_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(fake_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            fake_features.append(features)
            labels.append(1)  # 1: fake

# Convert to numpy arrays
X = np.array(real_features + fake_features)
y = np.array(labels)

# --- 4. Split Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- 5. Build the LSTM Model ---
model = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(64),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# --- 6. Compile the Model ---
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# --- 7. Train the Model ---
epochs = 10
batch_size = 32
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)

# --- 8. Evaluate the Model ---
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# --- 9. Function to Predict a Single Audio File ---
def predict_single_audio_lstm(audio_path, model, max_len=150, n_mfcc=20, frame_length=2048, hop_length=512):
    features = extract_sequential_features(audio_path, n_mfcc, frame_length, hop_length, max_len)
    if features is not None:
        features = features.reshape(1, features.shape[0], features.shape[1]) # Add batch dimension
        prediction = model.predict(features)[0][0]
        if prediction > 0.5:
            return "FAKE", prediction
        else:
            return "REAL", 1 - prediction
    else:
        return "Error", -1

# --- 10. Example Usage for a Specific File ---
specific_audio_path = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL/musk-original.wav"
result, probability = predict_single_audio_lstm(specific_audio_path, model)
print(f"\nPrediction for {os.path.basename(specific_audio_path)}: {result} (Probability of Fake: {probability:.2f})")

import librosa
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import IPython.display as ipd

# --- 1. Define Data Paths ---
real_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL"
fake_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE"

# --- 2. Function to Extract Sequential Features (e.g., MFCCs) ---
def extract_sequential_features(audio_path, n_mfcc=20, frame_length=2048, hop_length=512, max_len=150):
    try:
        y, sr = librosa.load(audio_path, duration=3)  # Example duration
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=frame_length, hop_length=hop_length)
        mfccs_transposed = mfccs.T

        # Pad or truncate sequences to a fixed length
        if mfccs_transposed.shape[0] < max_len:
            padding = np.zeros((max_len - mfccs_transposed.shape[0], n_mfcc))
            return np.vstack((mfccs_transposed, padding))
        else:
            return mfccs_transposed[:max_len]
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None

# --- 3. Load Data and Extract Features ---
real_features = []
fake_features = []
labels = []

for filename in os.listdir(real_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(real_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            real_features.append(features)
            labels.append(0)  # 0: real

for filename in os.listdir(fake_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(fake_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            fake_features.append(features)
            labels.append(1)  # 1: fake

# Convert to numpy arrays
X = np.array(real_features + fake_features)
y = np.array(labels)

# --- 4. Split Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- 5. Build the LSTM Model ---
model = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(64),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# --- 6. Compile the Model ---
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# --- 7. Train the Model ---
epochs = 10
batch_size = 32
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)

# --- 8. Function to Predict and Play a Single Audio File ---
def predict_and_play_lstm(audio_path, model, max_len=150, n_mfcc=20, frame_length=2048, hop_length=512):
    features = extract_sequential_features(audio_path, n_mfcc, frame_length, hop_length, max_len)
    if features is not None:
        features = features.reshape(1, features.shape[0], features.shape[1])  # Add batch dimension
        prediction = model.predict(features)[0][0]
        probability = prediction  # Probability of being fake (sigmoid output)

        if prediction > 0.5:
            result = "FAKE"
        else:
            result = "REAL"

        print(f"Audio: {os.path.basename(audio_path)}")
        print(f"Prediction: {result} (Probability of Fake: {probability:.2f})")
        ipd.display(ipd.Audio(audio_path))  # Play the audio
    else:
        print(f"Could not process audio file: {os.path.basename(audio_path)}")

# --- 9. Example Usage for a Specific File ---
specific_audio_path = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL/musk-original.wav"
predict_and_play_lstm(specific_audio_path, model)

# --- 10. Evaluate Overall Performance (Optional) ---
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

import librosa
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
import IPython.display as ipd

# --- 1. Define Data Paths ---
real_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL"
fake_audio_dir = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/FAKE"

# --- 2. Function to Extract Sequential Features (e.g., MFCCs) ---
def extract_sequential_features(audio_path, n_mfcc=20, frame_length=2048, hop_length=512, max_len=150):
    try:
        y, sr = librosa.load(audio_path, duration=3)  # Example duration
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=frame_length, hop_length=hop_length)
        mfccs_transposed = mfccs.T

        # Pad or truncate sequences to a fixed length
        if mfccs_transposed.shape[0] < max_len:
            padding = np.zeros((max_len - mfccs_transposed.shape[0], n_mfcc))
            return np.vstack((mfccs_transposed, padding))
        else:
            return mfccs_transposed[:max_len]
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None

# --- 3. Load Data and Extract Features ---
real_features = []
fake_features = []
labels = []

for filename in os.listdir(real_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(real_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            real_features.append(features)
            labels.append(0)  # 0: real

for filename in os.listdir(fake_audio_dir):
    if filename.endswith(".wav"):
        audio_path = os.path.join(fake_audio_dir, filename)
        features = extract_sequential_features(audio_path)
        if features is not None:
            fake_features.append(features)
            labels.append(1)  # 1: fake

# Convert to numpy arrays
X = np.array(real_features + fake_features)
y = np.array(labels)

# --- 4. Split Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# --- 5. Build the GRU Model ---
model = Sequential([
    GRU(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    GRU(64),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# --- 6. Compile the Model ---
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# --- 7. Train the Model ---
epochs = 10
batch_size = 32
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)

# --- 8. Function to Predict and Play a Single Audio File ---
def predict_and_play_gru(audio_path, model, max_len=150, n_mfcc=20, frame_length=2048, hop_length=512):
    features = extract_sequential_features(audio_path, n_mfcc, frame_length, hop_length, max_len)
    if features is not None:
        features = features.reshape(1, features.shape[0], features.shape[1])  # Add batch dimension
        prediction = model.predict(features)[0][0]
        probability = prediction  # Probability of being fake (sigmoid output)

        if prediction > 0.5:
            result = "FAKE"
        else:
            result = "REAL"

        print(f"Audio: {os.path.basename(audio_path)}")
        print(f"Prediction: {result} (Probability of Fake: {probability:.2f})")
        ipd.display(ipd.Audio(audio_path))  # Play the audio
    else:
        print(f"Could not process audio file: {os.path.basename(audio_path)}")

# --- 9. Example Usage for a Specific File ---
specific_audio_path = "/content/drive/MyDrive/Deep Fake Voice /KAGGLE/AUDIO/REAL/ryan-original.wav"
predict_and_play_gru(specific_audio_path, model)

# --- 10. Evaluate Overall Performance (Optional) ---
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")